<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="assets/css/main.css">
    <title>RoboPack | Learning Tactile-Informed Dynamics Models for Dense Packing</title>
</head>
<body>

<!--<div class="full-page-image">-->
<!--    <video id="bg-video" autoplay loop muted playsinline>-->
<!--        <source src="assets/videos/full_screen.mp4" type="video/mp4">-->
<!--    </video>-->
<!--    <div class="overlay"></div>-->
<!--    <div class="content" style="padding: 0 20px">-->
<!--        <h1>T<span style="font-variant-caps:all-small-caps;">RANSIC</span>: Sim-to-Real Policy Transfer by Learning from-->
<!--            Online Correction</h1>-->
<!--        <p>An RL sim-to-real policy trained with T<span style="font-variant-caps:all-small-caps;">RANSIC</span>-->
<!--            successfully-->
<!--            completes a long-horizon and contact-rich task: assembling a table lamp from scratch.</p>-->
<!--    </div>-->
<!--</div>-->

<div id="title_slide">
    <div class="title_left">
        <h1>RoboPack: Learning Tactile-Informed <br>Dynamics Models for Dense Packing</h1>

        <div style="height: 1.5vw;"></div>
        <div class="author-container">
            <div class="author-name"><a href="http://albertboai.com/" target="_blank">Bo Ai<sup>1,3</sup>*</a></div>
            <div class="author-name"><a href="https://s-tian.github.io/" target="_blank">Stephen Tian<sup>1</sup>*</a></div>
            <div class="author-name"><a href="https://hshi74.github.io/" target="_blank">Haochen Shi<sup>1</sup></a></div>
            <div class="author-name"><a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang<sup>2</sup></a></div>
        </div>
        <div class="author-container">
            <div class="author-name"><a href="https://www.linkedin.com/in/cheston-tan/" target="_blank">Cheston Tan<sup>3</sup></a></div>
            <div class="author-name"><a href="https://yunzhuli.github.io/" target="_blank">Yunzhu Li<sup>2</sup></a></div>
            <div class="author-name"><a href="https://jiajunwu.com/" target="_blank">Jiajun Wu<sup>1</sup></a></div>
        </div>
        <div class="author-container">
            <div class="author-name">(* equal contribution)</div>
        </div>

        <div class="affiliation">
            <p><sup>1</sup>Stanford University, USA</p>
            <p><sup>2</sup>University of Illinois at Urbana-Champaign, USA</p>
            <p><sup>3</sup>Agency for Science, Technology and Research, Singapore</p>
        </div>

        <div class="venue">
            <p> RSS 2024 </p>
        </div>

        <div style="height: 1.5vw;"></div>

        <div class="button-container">
            <a href="https://www.roboticsproceedings.org/rss20/p130.pdf" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>-->
            <a href="https://x.com/BoAi0110/status/1808433578067288490" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="https://github.com/BoAi01/robopack" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>-->
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-robot-astromech"></i>&emsp14;Models</a>-->
        </div>

        <br>

        <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/1-2-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a red coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/1-1-16x.mp4" type="video/mp4">
                </video>
                <div class="text">Pushing a purple box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/1-3-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a red coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/1-3-16x.mp4" type="video/mp4">
                </video>
                <div class="text">Pushing a pink box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/3-1-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a green coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/2-3-16x.mp4" type="video/mp4">
                </video>
                <div class="text">Pushing a pink box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/3-3-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a green coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/3-3-16x.mp4" type="video/mp4">
                </video>
                <div class="text">Pushing a purple box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <!-- Next and previous buttons -->
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
        </div>
        <br>

        <!-- The dots/circles -->
        <div style="text-align:center">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
            <span class="dot" onclick="currentSlide(3)"></span>
            <span class="dot" onclick="currentSlide(4)"></span>
            <span class="dot" onclick="currentSlide(5)"></span>
            <span class="dot" onclick="currentSlide(6)"></span>
            <span class="dot" onclick="currentSlide(7)"></span>
            <span class="dot" onclick="currentSlide(8)"></span>
        </div>
        <div id="abstract">
            <h1>Abstract</h1>
            <p>
                Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.
            </p>
        </div>
    </div>
</div>

<hr class="rounded">

<div id="overview">
    <h1>Motivation</h1>
    <!--        <embed src="assets/img/teaser-2.pdf" width="800px" height="2100px" />-->
    <div class="video_container">
        <video loop autoplay muted playsinline preload="metadata">
            <source src="assets/videos/human-dense-packing.m4v" type="video/mp4">
        </video>
        <div class="caption">
            <p>
                How we humans perform dense object packing.
            </p>
        </div>
    </div>

    <p>
        Imagine packing an item into a nearly full suitcase. As humans, we typically first form a visual representation
        of the scene and then make attempts to insert the object, <em><b>feeling</b></em> the compliance of the objects already
        inside to decide where and how to insert the new object. If a particular region feels soft, we can then apply
        additional force to make space and squeeze the new object in. This process is natural for us humans but very
        challenging for current robotic systems.
    </p>


    <div class="video_container">
        <video loop autoplay muted playsinline preload="metadata">
            <source src="assets/videos/human-daily-manipulation-2x2.m4v" type="video/mp4">
        </video>
        <div class="caption">
            <p>
                Humans performing a wide range of complex daily manipulation tasks. Our ability to integrate visual,
                tactile sensing and make future predictions is crucial for many daily tasks. For example, when we peel
                an orange, we sense how thick the peel is to determine how much force to apply.
            </p>
        </div>
    </div>

    <p>
        In this work, we seek to endow robots the capability to understand physical environments comprehensively with
        tactile and visual sensing.
    </p>

    <h1>Method Overview</h1>
    <p>
        <b>
            RoboPack is a framework that integrates tactile-informed state estimation, dynamics prediction with model
            predictive control for manipulating objects with unknown physical properties.
        </b>
    </p>

    <p>
        At the core of our framework is a particle-based representation that contains visual, tactile, and physics
        information.
<!--        We first extract particles from the scene, then encode tactile reading, as well as, a-->
<!--        learned latent object-level physics vector as particle attributes.-->
    </p>

    <div class="video_container">
        <video loop autoplay muted playsinline preload="metadata">
            <source src="assets/videos/particle-representation.m4v" type="video/mp4">
        </video>
        <div class="caption">
            <p>
                Illustration of constructing our particle-based scene representation.
            </p>
        </div>
    </div>

    <p>
        With this representation, we are able to learn state estimation and dynamics prediction for planning.
    </p>

    <div class="video_container">
        <video loop autoplay muted playsinline preload="metadata">
            <source src="assets/videos/estimation-planning.m4v" type="video/mp4">
        </video>
        <div class="caption">
            <p>
                High-level sketch of our state estimation, dynamics learning, and planning pipeline.
            </p>
        </div>
    </div>

    <p>
        One feature of this framework is that, we only need one visual frame to initialize the system, and only
        tactile feedback is required throughout the episode. This resembles humans' ability of manipulating objects
        without really looking at the scene (e.g., when talking to a person), and effectively sidesteps the difficulty
        of obtaining complete visual observation of the world when severe visual occlusions are present.
    </p>

<!--    <p>-->
<!--        For a more technically accurate and detailed description of our framework, please refer to our paper. What is-->
<!--        not covered here includes our Dino Field-based tracking system, a mathematical description of our (recurrent)-->
<!--        graph neural network-based state estimator and dynamics predictor, and the implementation of our-->
<!--        model-predictive controller.-->
<!--    </p>-->

    <h1>Experimental Setup</h1>
    <p>
        We compare our approach against baselines, including prior work on dynamics learning
        (<a href="https://hshi74.github.io/robocook/">RoboCook</a>),
        ablated versions of our method (w/o tactile, w/o state estimation), and
        physics simulator-based planner on two challenging tasks:
    </p>
    <p>
        1. <em><b>Non-Prehensile Box Pushing</b></em>: The robot holds a tool, e.g., a rod, to push a box to a target
        pose. It is much more challenging than a normal object pushing task, because (1) the robot gripper is very
        compliant and the tool is only loosely held, making the interaction between tool and box complex, and (2)
        we manually placed calibration weights inside the box to adjust its mass distribution, making dynamics highly
        uncertain.
    </p>
    <p>
        2. <em><b>Dense Packing</b></em>: The robot places an object inside a visually densely packed box. It needs to
        interact with the environment to understand the physics properties (e.g., movability and deformability) of objects
        inside the box to create space with suitable actions in order to place the object in the free space.
    </p>

    <h1>Dynamics Prediction Performance</h1>
    <p>
        We first evaluate our method on dynamics prediction for two tasks.
<!--        It can be observed that our method outperforms other methods quantitatively and qualitatively.-->
    </p>

    <div class="video_container" style="text-align: center">
            <img src="assets/img/dynamics-prediction-quantitative.png" style="width: 70%;">
            <div class="caption" style="text-align: left">
                <p>
                    Long-horizon dynamics prediction results on the two task datasets.
                    Errors represent a 95% confidence interval.
                </p>
            </div>
        </div>

    <div class="video_container" style="text-align: center">
            <img src="assets/img/dynamics-prediction-qualitative.png" style="width: 70%;">
            <div class="caption" style="text-align: left">
                <p>
                    Qualitative results on dynamics prediction.
<!--                    Predictions made by our model compared to baseline methods in the Non-prehensile Box Pushing task.-->
<!--                    Red dots indicate the rod and blue dots represent the box.-->
<!--                    Our method closely approximates the ground truth and outperforms all the baseline methods.-->
<!--                    For visualization, the blue dashed lines outline box contours and red dashed lines show-->
<!--                    in-hand object contours.-->
                </p>
            </div>
        </div>

    <h1>Analysis of Learned Physics Parameters</h1>
    <p>
        <b>What do the latent physics vectors learn?</b> To answer this question, we collect the latent vectors generated
        during the box pushing task and perform a clustering analysis.
    </p>
    <p>
        PCA visualizations at four distinct timesteps show that the physics parameters gradually form clusters by box type. We also employ a linear classifier trained on these parameters to accurately predict box types to demonstrate these clusters’ linear separability. The classifier’s improving accuracy across timesteps underscores the state estimator’s proficiency in extracting and integrating box-specific information from the tactile observation history.
    </p>


    <div class="video_container" style="text-align: center">
            <img src="assets/img/physics_params.png" style="width: 90%;">
            <div class="caption" style="text-align: left">
                <p>
                    Analysis of learned physics parameters.
                </p>
            </div>
        </div>


    <h1>Benchmarking Real-World Planning Performance</h1>
    <p>
        We first present quantitative results on box pushing. It can be observed
        that, at every time step, our method is the one that is closest to the goal, indicating that our method is able to consistently find the most efficient pushes.
<!--        This showcases that our method can identify the correct physics parameters for accurate dynamics prediction.-->
    </p>

    <div class="approach">
        <div class="video_container" style="text-align: center">
            <img src="assets/img/pushing_lineplot.png" style="width: 60%;">
            <div class="caption" style="text-align: left">
                <p>
                    Real-world planning performance on the box pushing task. Shaded regions denote the first and third quantiles.
<!--                    Note that different methods generally perform well on easier cases, leading to overlap between shadow regions. Our method has stable performance even for hard ones: its 75-percentile error is lower than the mean error of all other method.-->
                </p>
            </div>
        </div>
    </div>

    <p>
        The table below presents more statistics for the box pushing experiments.
    </p>

    <div class="video_container" style="text-align: center">
            <img src="assets/img/pushing_results.png" style="width: 100%;">
            <div class="caption" style="text-align: left">
                <p>
                    Per-configuration results on the non-prehensile box pushing task.
<!--                    We report the minimum error to goal across 10 plan executions per trial, trial success rates, and number of execution steps to solve the task. A trial is labeled as a success if it achieves an error lower than 0.02 for point-wise MSE within 10 pushes.-->
                </p>
            </div>
        </div>

    <p>
        For the dense packing task, we compare our method against the strongest baseline in the pushing experiments.
        Our method is able to consistently achieve 2x as high success rate as the baseline across seen and unseen object sets.
<!--        , showcasing the effectiveness and generalizability of our approach.-->
    </p>

    <div class="video_container" style="text-align: center">
            <img src="assets/img/packing_results.png" style="width: 60%;">
            <div class="caption" style="text-align: left">
                <p>
                    Success rates on the dense packing task.
<!--                    In the Unseen Objects setting, half or more of the objects in the tray are unseen. A trial is considered successful if the robot correctly determines feasible insertion locations and creates enough space (through deformation) to pack the object. The robot automatically attempts to pack the object when its end effector y-position exceeds a given threshold.-->
                </p>
            </div>
        </div>


    <h1>More Qualitative Results</h1>
    <div style="height: 1.5vw;"></div>

    <div class="video_container">
        <video loop autoplay muted playsinline preload="metadata">
            <source src="assets/videos/pushing-3x3.mp4" type="video/mp4">
        </video>
        <div class="caption">
            <p>
                Pushing boxes with various mass distributions using a loosely held tool.
            </p>
        </div>
    </div>

    <div style="height: 1.5vw;"></div>

    <div class="video_container">
        <video loop autoplay muted playsinline preload="metadata">
            <source src="assets/videos/packing-3x3.mp4" type="video/mp4">
        </video>
        <div class="caption">
            <p>
                Packing objects into boxes in different layouts.
            </p>
        </div>
    </div>

    <h1>Conclusion</h1>

    <p>
        We presented RoboPack, a framework for learning tactile-informed dynamics models for manipulating objects
        in multi-object scenes with varied physical properties. By integrating information from prior interactions
        from a compliant visual tactile sensor, our method adaptively updates estimated latent physics parameters,
        resulting in improved physical prediction and downstream planning performance. We hope that this is a step towards
        robots that can seamlessly integrate information with multiple modalities from their environments to guide their
        decision-making.
    </p>

    <h1>BibTeX</h1>
<!--    <p>-->
<!--        Coming soon.-->
<!--    </p>-->
    <p class="bibtex">
        @article{ai2024robopack,<br>
              &nbsp;&nbsp;title={RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing}, <br>
              &nbsp;&nbsp;author={Bo Ai and Stephen Tian and Haochen Shi and Yixuan Wang and Cheston Tan and Yunzhu Li and Jiajun Wu}, <br>
              &nbsp;&nbsp;journal={Robotics: Science and Systems (RSS)},<br>
              &nbsp;&nbsp;year={2024}, <br>
              &nbsp;&nbsp;url={https://arxiv.org/abs/2407.01418},<br>
        }
    </p>
    <br>
</div>
</body>

<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>
</html>
