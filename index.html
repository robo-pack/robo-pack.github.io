<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="assets/css/main.css">
    <title>RoboPack | Learning Tactile-Informed Dynamics Models for Dense Packing</title>
</head>
<body>

<!--<div class="full-page-image">-->
<!--    <video id="bg-video" autoplay loop muted playsinline>-->
<!--        <source src="assets/videos/full_screen.mp4" type="video/mp4">-->
<!--    </video>-->
<!--    <div class="overlay"></div>-->
<!--    <div class="content" style="padding: 0 20px">-->
<!--        <h1>T<span style="font-variant-caps:all-small-caps;">RANSIC</span>: Sim-to-Real Policy Transfer by Learning from-->
<!--            Online Correction</h1>-->
<!--        <p>An RL sim-to-real policy trained with T<span style="font-variant-caps:all-small-caps;">RANSIC</span>-->
<!--            successfully-->
<!--            completes a long-horizon and contact-rich task: assembling a table lamp from scratch.</p>-->
<!--    </div>-->
<!--</div>-->

<div id="title_slide">
    <div class="title_left">
        <h1>RoboPack: Learning Tactile-Informed <br>Dynamics Models for Dense Packing</h1>

        <div style="height: 1.5vw;"></div>
        <div class="author-container">
            <div class="author-name"><a href="http://albertboai.com/" target="_blank">Bo Ai*<sup>1,2,4</sup></a></div>
            <div class="author-name"><a href="https://s-tian.github.io/" target="_blank">Stephen Tian*<sup>2</sup></a></div>
            <div class="author-name"><a href="https://hshi74.github.io/" target="_blank">Haochen Shi<sup>2</sup></a></div>
            <div class="author-name"><a href="https://wangyixuan12.github.io/" target="_blank">Yixuan Wang<sup>3</sup></a></div>
        </div>
        <div class="author-container">
            <div class="author-name"><a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan" target="_blank">Cheston Tan<sup>1</sup></a></div>
            <div class="author-name"><a href="https://yunzhuli.github.io/" target="_blank">Yunzhu Li<sup>3</sup></a></div>
            <div class="author-name"><a href="https://jiajunwu.com/" target="_blank">Jiajun Wu<sup>2</sup></a></div>
        </div>
        <div class="author-container">
            <div class="author-name">(* equal contribution)</div>
        </div>

        <div class="affiliation">
            <p><sup>1</sup> Agency for Science, Technology and Research (A*STAR), Singapore</p>
            <p><sup>2</sup> Stanford University, USA</p>
            <p><sup>3</sup> University of Illinois at Urbana-Champaign, USA</p>
            <p><sup>4</sup> National University of Singapore, Singapore</p>
        </div>
        <div style="height: 1.5vw;"></div>

<!--        <div class="button-container">-->
<!--            <a href="" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>-->
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>-->
<!--            <a href="" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>-->
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a>-->
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>-->
<!--            <a href="" target="_blank" class="button"><i class="fa-light fa-robot-astromech"></i>&emsp14;Models</a>-->
<!--        </div>-->

        <br>

        <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/1-2-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a red coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/1-1-16x.m4v" type="video/mp4">
                </video>
                <div class="text">Pushing a purple box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/1-3-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a red coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/1-3-16x.m4v" type="video/mp4">
                </video>
                <div class="text">Pushing a pink box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/3-1-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a green coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/2-3-16x.m4v" type="video/mp4">
                </video>
                <div class="text">Pushing a pink box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/packing/3-3-12x.mp4" type="video/mp4">
                </video>
                <div class="text">Placing a green coke can into a tray using only tactile feedback.</div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pushing/3-3-16x.m4v" type="video/mp4">
                </video>
                <div class="text">Pushing a purple box with an unobserved mass distribution using visual-tactile feedback.</div>
            </div>

            <!-- Next and previous buttons -->
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
        </div>
        <br>

        <!-- The dots/circles -->
        <div style="text-align:center">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
            <span class="dot" onclick="currentSlide(3)"></span>
            <span class="dot" onclick="currentSlide(4)"></span>
            <span class="dot" onclick="currentSlide(5)"></span>
            <span class="dot" onclick="currentSlide(6)"></span>
            <span class="dot" onclick="currentSlide(7)"></span>
            <span class="dot" onclick="currentSlide(8)"></span>
        </div>
        <div id="abstract">
            <h1>Abstract</h1>
            <p>
                Tactile feedback is critical for understanding the dynamics of both rigid and deformable objects in many manipulation tasks, such as non-prehensile manipulation and dense packing. We introduce an approach that combines visual and tactile sensing for robotic manipulation by learning a neural, tactile-informed dynamics model. Our proposed framework, RoboPack, employs a recurrent graph neural network to estimate object states, including particles and object-level latent physics information, from historical visuo-tactile observations and to perform future state predictions. Our tactile-informed dynamics model, learned from real-world data, can solve downstream robotics tasks with model-predictive control. We demonstrate our approach on a real robot equipped with a compliant Soft-Bubble tactile sensor on non-prehensile manipulation and dense packing tasks, where the robot must infer the physics properties of objects from direct and indirect interactions. Trained on only an average of 30 minutes of real-world interaction data per task, our model can perform online adaptation and make touch-informed predictions. Through extensive evaluations in both long-horizon dynamics prediction and real-world manipulation, our method demonstrates superior effectiveness compared to previous learning-based and physics-based simulation systems.
            </p>
        </div>
    </div>
</div>
<hr class="rounded">
<div id="overview">
    <h1>Method Overview</h1>
    <p>
        An overview of T<span style="font-variant-caps:all-small-caps;">RANSIC</span> is shown below. At a high level,
        after training the base policy in simulation, we deploy it on the real robot while monitored by a human
        operator. The human interrupts the autonomous execution when necessary and provides online correction through
        teleoperation. Such intervention and online correction are collected to train a residual policy, after which
        both base and residual policies are deployed to complete contact-rich manipulation tasks.
    </p>
    <div class="allegrofail">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/videos/method_overview.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>
                    <b>(a)</b> Base policies are first trained in simulation through <i>action space distillation</i>
                    with demonstrations generated by RL teacher policies. Base policies take point cloud as input to
                    reduce perception gap. <b>(b)</b> After acquiring base policies, they are first deployed where a
                    human operator monitors the execution. The human intervenes and corrects through teleoperation when
                    necessary. Such intervention and correction data are collected to learn <i>residual policies</i>.
                    Finally, both residual policies and base policies are integrated during test time to achieve
                    successful transfer.
                </p>
            </div>
        </div>
    </div>

    <h1>Residual Policy Learning from Human Correction to Bridge Sim-to-Real Gaps</h1>
    <p>
        Our key insight is that the human-in-the-loop framework is promising for addressing the sim-to-real gaps as a
        whole, in which humans directly assist the physical robots during policy execution by providing online
        correction signals. The knowledge required to close sim-to-real gaps can be learned from human signals. In
        T<span style="font-variant-caps:all-small-caps;">RANSIC</span>, once the base robot policies are acquired from
        simulation training, they are deployed onto real robots where human operators monitor the execution.
        When the robot makes mistakes or gets stuck, humans interrupt and assist robot policies through teleoperation.
        Such human intervention data are collected to train a <i>residual policy</i>, after which the base policy and
        the residual policy are combined to solve contact-rich manipulation tasks, such as inserting a table leg into a
        tabletop and screwing a light bulb into the lamp base. With the synergetic integration with previous approaches,
        since humans can successfully assist the robot trained in silico to complete real-world tasks, sim-to-real gaps
        are implicitly handled and addressed by humans in a domain-agnostic manner. Additionally, human supervision
        naturally guarantees safe deployment.
    </p>
    <div class="approach">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/videos/residual_policy.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>
                    Simulation policies are deployed where a human operator monitors the execution. The human intervenes
                    and corrects through teleoperation when necessary. Such intervention and correction data are
                    collected to learn <i>residual policies</i>. Finally, both residual policies and simulation policies
                    are integrated during test time to achieve successful transfer.
                </p>
            </div>
        </div>
    </div>

    <h1>Large-Scale Simulation Training to Acquire Base Policies</h1>
    <p>
        Using the state-of-the-art simulation technique, we train base policies in simulation with hundreds of thousands
        of frames per second. This greatly alleivates the human burden for data collection. We first train teacher
        policies with model-free reinforcement learning (RL) on massively parallelized environments. We then distill RL
        teacher policies into student visuomotor policies.
    </p>

    <br>
    <div class="approach">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/videos/sim.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>
                    For each manipulation skill, we first train an RL policy and then distill into a visuomotor policy.
                    We apply domain randomization such that trained simulation policies are robust enough. Several
                    important design choices are made to facilitate sim-to-real transfer, such as taking point cloud
                    inputs and adopting joint position actions.
                </p>
            </div>
        </div>
    </div>

    <h1>Visuomotor Policies with Point Cloud Observations and Joint Position Actions</h1>

    <p>
        Object geometry matters for contact-rich manipulation. For example, a robot should ideally insert a light bulb
        into the lamp base with the thread facing down. To retain such 3D information and facilitate sim-to-real
        transfer, we propose to use point cloud as the main visual modality. Typical RGB observation used in visuomotor
        policy training suffers from several drawbacks that hinder successful transfer, such as the vulnerability to
        different camera poses and discrepancies between synthetic and real images. Well-calibrated point cloud
        observation can bypass these issues.
        <br>
        A suitable action abstraction is critical for efficient learning as well as sim-to-real transfer. A high-level
        controller such as the operational space controller (OSC) facilitates RL exploration but may hinder sim-to-real
        transfer because it requires accurate modeling of robot parameters, such as joint friction, mass, and inertia;
        on the other hand, a low-level action space such as the joint position ensures consistent deployment in
        simulation and real hardware, but renders trial-and-error RL impractical. We draw inspiration from the
        teacher-student framework and propose to first train the teacher policy with OSC and then distill successful
        trajectories into the student policy with joint position control. We name this approach as <i>action space
        distillation</i> and find it crucial to overcome the sim-to-real controller gap.
    </p>
    <br>
    <div class="approach">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/videos/pcd_grid.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>
                    We use point cloud as the main visual modality. Simulation polices are trained on downsampled
                    synthetic point-cloud observations. They are able to transfer to real-world point-cloud observations
                    captured by standard depth cameras.
                </p>
            </div>
        </div>
    </div>


    <h1>Experiments</h1>
    <p>We seek to answer the following research questions with our experiments:</p>
    <details>
        <summary>Research Questions</summary>
        <p>
            <i>Q1</i>: Does T<span style="font-variant-caps:all-small-caps;">RANSIC</span> lead to better transfer
            performance compared to traditional sim-to-real methods?
            <br>
            <i>Q2</i>: Can T<span style="font-variant-caps:all-small-caps;">RANSIC</span> better integrate human
            correction into the policy learned in simulation than existing interactive imitation learning (IL)
            approaches?
            <br>
            <i>Q3</i>: Does T<span style="font-variant-caps:all-small-caps;">RANSIC</span> require less real-world data
            to achieve good performance compared to algorithms that only learn from real-robot trajectories?
            <br>
            <i>Q4</i>: How effective can T<span style="font-variant-caps:all-small-caps;">RANSIC</span> address
            different types of sim-to-real gaps?
            <br>
            <i>Q5</i>: How does T<span style="font-variant-caps:all-small-caps;">RANSIC</span> scale with human effort?
            <br>
            <i>Q6</i>: Does T<span style="font-variant-caps:all-small-caps;">RANSIC</span> exhibit intriguing
            properties, such as generalization to unseen objects, effective gating, policy robustness, consistency in
            learned visual features, ability to solve long-horizon manipulation tasks, and other emergent behaviors?

        </p>
    </details>
    <p>
        We consider complex contact-rich manipulation tasks in FurnitureBench that require high precision. Specifically,
        we divide the assembly of a square table into four independent tasks: <i>Stabilize</i>, <i>Reach and Grasp</i>,
        <i>Insert</i>, and <i>Screw</i>. We collect 20, 100, 90, and 17 real-robot trajectories with human correction,
        respectively. On average, these amount to 260 interventions for each task.
    </p>

    <br>

    <div class="approach">
        <div class="video_container">
            <img src="assets/img/experiment_main.png">
            <div class="caption">
                <p>
                    <b>Average success rates over four benchmarked tasks.</b> T<span
                        style="font-variant-caps:all-small-caps;">RANSIC</span> significantly outperforms three baseline
                    groups. They are 1) traditional sim-to-real approaches, such as domain randomization and data
                    augmentation (“DR. & Data Aug.”) and real-world fine-tuning; 2) interactive imitation learning
                    methods, such as HG-Dagger and IWR; and 3) approaches that only train on real-robot data, such as
                    BC, BC-RNN, and IQL. Results are success rates averaged over four tasks. Each evaluation consists of
                    20 trials with different initial settings. We make our best efforts to ensure the same initial
                    configuration when evaluating different methods.
                </p>
            </div>
        </div>
    </div>

    <br>

    <div class="approach">
        <div class="video_container">
            <img src="assets/img/experiment_main_table.png">
            <div class="caption">
                <p>
                    <b>Success rates per tasks.</b> T<span style="font-variant-caps:all-small-caps;">RANSIC</span>
                    outperforms all baseline methods on all four tasks.
                </p>
            </div>
        </div>
    </div>

    <br>

    <p>
        We show that in sim-to-real transfer, a good base policy learned from the simulation can be combined with
        limited real-world data to achieve success (<i>Q3</i>). However, effectively utilizing human correction data to
        address the sim-to-real gap is challenging (<i>Q1</i>), especially when we want to prevent catastrophic
        forgetting of the base policy (<i>Q2</i>).
    </p>

    <h1>Effectiveness in Addressing Different Sim-to-Real Gaps</h1>
    <p>
        While T<span style="font-variant-caps:all-small-caps;">RANSIC</span> is a holistic approach to address multiple
        sim-to-real gaps simultaneously, we shed light on its ability to close each individual gap. To do so, we create
        five different simulation-reality pairs. For each of them, we intentionally create large gaps between the
        simulation and the real world. These gaps are applied to the real-world setting and they include <i>perception
        error</i>, <i>underactuated controller</i>, <i>embodiment mismatch</i>, <i>dynamics difference</i>, and <i>object
        asset mismatch</i>.
    </p>
    <div class="approach">
        <div class="video_container" style="text-align: center">
            <img src="assets/img/experiment_different_sim2real_gaps.png" style="width: 60%;">
            <div class="caption" style="text-align: left">
                <p>
                    <b>Robustness to different sim-to-real gaps.</b> Numbers are averaged success rates (%). Polar bars
                    represent performances after training with data collected specifically to address a particular gap.
                    Dashed lines are zero-shot performances. Shaded circles show average performances across five pairs.
                </p>
            </div>
        </div>
    </div>

    <br>

    <p>
        T<span style="font-variant-caps:all-small-caps;">RANSIC</span> achieves an average success rate of 77% across
        five different simulation-reality pairs with deliberately exacerbated sim-to-real gaps. This indicates its
        remarkable ability to close these individual gaps. In contrast, the best baseline method, IWR, only achieves an
        average success rate of 18%. We attribute this effectiveness in addressing different sim-to-real gaps to the
        residual policy design.
    </p>


    <h1>Scalability with Human Effort</h1>
    <p>
        Scaling with human effort is a desired property for human-in-the-loop robot learning methods. We show that
        T<span style="font-variant-caps:all-small-caps;">RANSIC</span> has better human data scalability than the best
        baseline IWR. If we increase the size of the correction dataset from 25% to 75% of the full dataset size, T<span
            style="font-variant-caps:all-small-caps;">RANSIC</span> achieves a relative improvement of 42% in the
        average success rate. In contrast, IWR only achieves 23% relative improvement. Additionally, IWR performance
        plateaus at an early stage and even starts to decrease as more human data becomes available. We hypothesize that
        IWR suffers from catastrophic forgetting and struggles to properly model the behavioral modes of humans and
        trained robots. On the other hand, T<span style="font-variant-caps:all-small-caps;">RANSIC</span> bypasses these
        issues by learning gated residual policies only from human correction.
    </p>

    <br>

    <div class="approach">
        <div class="video_container" style="text-align: center">
            <img src="assets/img/experiment_data_scalability.png" style="width: 50%;">
            <div class="caption" style="text-align: left">
                <p>
                    <b>Scalability with human correction data.</b> Numbers are success rates averaged over four tasks
                    with different amount of human correction data.
                </p>
            </div>
        </div>
    </div>

    <h1>Intriguing Properties and Emergent Behaviors</h1>

    <p>
        We examine further T<span style="font-variant-caps:all-small-caps;">RANSIC</span> and discuss several emergent
        capabilities. We show that 1) T<span style="font-variant-caps:all-small-caps;">RANSIC</span> can reliably
        operate in a fully autonomous setting once the gating mechanism is learned; 2) T<span
            style="font-variant-caps:all-small-caps;">RANSIC</span> is robust against partial point cloud observations
        and suboptimal correction data; and 3) T<span style="font-variant-caps:all-small-caps;">RANSIC</span> learns
        consistent visual features between the simulation and reality.
    </p>

    <br>

    <div class="approach">
        <div class="video_container" style="text-align: center">
            <img src="assets/img/experiment_ablation.png" style="width: 60%;">
            <div class="caption" style="text-align: left">
                <p>
                    <b>Results of ablation studies. </b> We study the effects of different gating mechanisms (learned
                    gating vs human gating), policy robustness against reduced cameras and suboptimal correction data,
                    and the importance of visual encoder regularization.
                </p>
            </div>
        </div>
    </div>

    <h1> Failure Cases </h1>
    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/videos/insert_failure.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/videos/bended_gripper.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/videos/unstable_grasp_pose.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p><b>Several failure cases.</b> For instances, they include inaccurate insertion, bended gripper,
                    unstable grasping pose, and over-screwing.</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/videos/over_screw.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Conclusion</h1>

    <p>
        In this work, we present T<span style="font-variant-caps:all-small-caps;">RANSIC</span>, a holistic
        human-in-the-loop method to tackle sim-to-real transfer of policies for contact-rich manipulation tasks. We show
        that in sim-to-real transfer, a good base policy learned from the simulation can be combined with limited
        real-world data to achieve success. However, effectively utilizing human correction data to address the
        sim-to-real gap is challenging, especially when we want to prevent catastrophic forgetting of the base policy. T<span
            style="font-variant-caps:all-small-caps;">RANSIC</span> successfully addresses these challenges by learning
        a gated residual policy from human correction data. We show that T<span
            style="font-variant-caps:all-small-caps;">RANSIC</span> is effective as a holistic approach to address
        different types of sim-to-real gaps when presented simultaneously; it is also effective as an approach to
        address individual gaps of very different natures. It displays attractive properties, such as scaling with human
        effort. The code is submitted as part of the supplementary materials and will be open-sourced to facilitate
        research in sim-to-real transfer for complex manipulation tasks.
    </p>

    <h1>BibTeX</h1>
    <p class="bibtex">tbd
    </p>
    <br>
</div>
</body>

<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>
</html>
